# This is an example of a workstation configuration file.

# The path to the directory where the data is stored
# (can also specify the path to a single fasta file)
fasta_path: /homes/ogokdemir/instruct-dpo/generated_sequences/proteins/2p5b_29kseqs_feb14.fasta

# The path to the directory where the output files will be stored.
# Will write a new subdirectory for each sequence containing the
# folded structure PDB file and a JSON file with the plddt score.
output_dir: /homes/ogokdemir/instruct-dpo/parslfold_outputs

# The number of sequences to fold per worker function call.
chunk_size: 1


# The folding parameters to use (these parameters are for chai1)
folding_config:
    # Accelerator platform to use for folding.
    device: cuda
    # HF pointer to the tokenizer.
    tokenizer: 'facebook/esmfold_v1'
    # HF pointer to the model.
    model: 'facebook/esmfold_v1'
    # Whether to use half precision at inference.
    use_float16: False
    # Whether to allow TF32 at inference (if GPU supports it).
    allow_tf32: False
    # The chunk size for the axial attention.
    chunk_size: null

# The compute configuration to use.
compute_config:
    # Specify we want the workstation parsl configuration
    name: workstation
    # Identify which GPUs to assign tasks to. It's generally recommended to first check
    # nvidia-smi to see which GPUs are available. The numbers below are analogous to
    # setting CUDA_VISIBLE_DEVICES=0
    available_accelerators: ["1", "2"]
